{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39d07724",
   "metadata": {},
   "outputs": [],
   "source": [
    "from grid_world import GridWorld, Action\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3c722fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.9\n",
    "rows = 5\n",
    "cols = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c72eb428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init grid:\n",
      "⬜️⬜️⬜️⬜️⬜️\n",
      "⬜️🚫🚫⬜️⬜️\n",
      "⬜️⬜️🚫⬜️⬜️\n",
      "⬜️🚫✅🚫⬜️\n",
      "⬜️🚫⬜️⬜️⬜️\n",
      "random policy:\n",
      "⬆️⬆️⬇️⬅️⬆️\n",
      "🔄⏫️⏪⬇️➡️\n",
      "⬇️➡️🔄⬅️➡️\n",
      "⬆️⏩️✅⏩️➡️\n",
      "➡️⏫️⬇️⬇️⬇️\n"
     ]
    }
   ],
   "source": [
    "env = GridWorld(\n",
    "    desc=[\".....\", \".##..\", \"..#..\", \".#T#.\", \".#...\"],\n",
    "    forbidden_score=-5,\n",
    "    terminal_score=1,\n",
    ")\n",
    "print(\"init grid:\")\n",
    "env.render_grid()\n",
    "# print(\"init policy:\")\n",
    "state_values = np.zeros(rows * cols)\n",
    "q_table = np.zeros((rows * cols, len(Action)))\n",
    "# policy = np.argmax(q_table, axis=1)\n",
    "# env.render_policy(policy)\n",
    "\n",
    "print(\"random policy:\")\n",
    "# policy = np.random.choice(len(Action), size=rows * cols)\n",
    "policy = np.eye(cols)[np.random.randint(0, 5, size=(rows * cols))]\n",
    "env.render_policy(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d17bdd6",
   "metadata": {},
   "source": [
    "### Every-visited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7931a1f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0 Euclidean Distance:125.0\n",
      "➡️➡️➡️➡️⬇️\n",
      "⬆️⏫️⏫️⬆️⬆️\n",
      "⬆️⬅️⏬⬆️⬆️\n",
      "⬆️⏩️✅⏪⬆️\n",
      "⬆️⏩️⬆️➡️⬆️\n",
      "action value's mean -11.763227538858368\n",
      "iter 1 Euclidean Distance:63342.37158094292\n",
      "➡️➡️➡️➡️⬇️\n",
      "⬆️⏫️⏫️⬆️⬆️\n",
      "⬆️⬅️⏬⬆️⬆️\n",
      "⬆️⏩️✅⏪⬆️\n",
      "⬆️⏩️⬆️➡️⬆️\n",
      "action value's mean -11.763227538858368\n",
      "iter 2 Euclidean Distance:0.0\n",
      "➡️➡️➡️➡️⬇️\n",
      "⬆️⏫️⏫️⬆️⬆️\n",
      "⬆️⬅️⏬⬆️⬆️\n",
      "⬆️⏩️✅⏪⬆️\n",
      "⬆️⏩️⬆️➡️⬆️\n",
      "action value's mean -11.763227538858368\n",
      "Converged at iteration 2\n"
     ]
    }
   ],
   "source": [
    "max_iters = 1000\n",
    "theta = 1e-3\n",
    "traj_steps = 100\n",
    "old_q_table = q_table.copy() + 1\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    delta = np.sum((old_q_table - q_table) ** 2)\n",
    "    print(f\"iter {iter} Euclidean Distance:{delta}\")\n",
    "\n",
    "    old_q_table = q_table.copy()\n",
    "    for state in range(env.get_state_space_size()):\n",
    "        for action in range(env.get_action_space_size()):\n",
    "            qtable_rewards = [\n",
    "                [0 for j in range(env.get_action_space_size())]\n",
    "                for i in range(env.get_state_space_size())\n",
    "            ]\n",
    "            qtable_nums = [\n",
    "                [0 for j in range(env.get_action_space_size())]\n",
    "                for i in range(env.get_state_space_size())\n",
    "            ]\n",
    "\n",
    "            traj = env.get_traj(state, action, policy, steps=traj_steps)\n",
    "\n",
    "            state, action, reward, next_state, next_action = traj[-1]\n",
    "            for k in range(traj_steps - 1, -1, -1):\n",
    "                tmp_state, tmp_action, tmp_score, _, _ = traj[k]\n",
    "                reward = reward * gamma + tmp_score\n",
    "                qtable_rewards[tmp_state][tmp_action] += reward\n",
    "                qtable_nums[tmp_state][tmp_action] += 1\n",
    "                q_table[tmp_state][tmp_action] = (\n",
    "                    qtable_rewards[tmp_state][tmp_action]\n",
    "                    / qtable_nums[tmp_state][tmp_action]\n",
    "                )\n",
    "\n",
    "    policy = np.eye(5)[np.argmax(q_table, axis=1)]\n",
    "    env.render_policy(policy)\n",
    "    print(\"action value's mean\", q_table.mean())\n",
    "\n",
    "    if delta < theta:\n",
    "        print(f\"Converged at iteration {iter}\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9401f89c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c8f520c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init grid:\n",
      "⬜️⬜️⬜️⬜️⬜️\n",
      "⬜️🚫🚫⬜️⬜️\n",
      "⬜️⬜️🚫⬜️⬜️\n",
      "⬜️🚫✅🚫⬜️\n",
      "⬜️🚫⬜️⬜️⬜️\n",
      "random policy:\n",
      "➡️➡️⬇️⬆️➡️\n",
      "⬇️🔄⏪⬅️⬇️\n",
      "🔄🔄🔄⬇️➡️\n",
      "⬅️🔄✅⏫️⬆️\n",
      "🔄🔄➡️🔄⬇️\n"
     ]
    }
   ],
   "source": [
    "env = GridWorld(\n",
    "    desc=[\".....\", \".##..\", \"..#..\", \".#T#.\", \".#...\"],\n",
    "    forbidden_score=-5,\n",
    "    terminal_score=1,\n",
    ")\n",
    "print(\"init grid:\")\n",
    "env.render_grid()\n",
    "# print(\"init policy:\")\n",
    "state_values = np.zeros(rows * cols)\n",
    "q_table = np.zeros((rows * cols, len(Action)))\n",
    "# policy = np.argmax(q_table, axis=1)\n",
    "# env.render_policy(policy)\n",
    "\n",
    "print(\"random policy:\")\n",
    "# policy = np.random.choice(len(Action), size=rows * cols)\n",
    "policy = np.eye(cols)[np.random.randint(0, 5, size=(rows * cols))]\n",
    "env.render_policy(policy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bca1f769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0 Euclidean Distance:125.0\n",
      "➡️➡️➡️➡️⬇️\n",
      "⬆️⏫️⏫️⬆️⬆️\n",
      "⬆️⬅️⏬⬆️⬆️\n",
      "⬆️⏩️✅⏪⬆️\n",
      "⬆️⏩️⬆️➡️⬆️\n",
      "iter 1 Euclidean Distance:74996.41425400978\n",
      "➡️➡️➡️➡️⬇️\n",
      "⬆️⏫️⏫️⬆️⬆️\n",
      "⬆️⬅️⏬⬆️⬆️\n",
      "⬆️⏩️✅⏪⬆️\n",
      "⬆️⏩️⬆️➡️⬆️\n",
      "iter 2 Euclidean Distance:0.0\n",
      "➡️➡️➡️➡️⬇️\n",
      "⬆️⏫️⏫️⬆️⬆️\n",
      "⬆️⬅️⏬⬆️⬆️\n",
      "⬆️⏩️✅⏪⬆️\n",
      "⬆️⏩️⬆️➡️⬆️\n",
      "Converged at iteration 2\n"
     ]
    }
   ],
   "source": [
    "max_iters = 1000\n",
    "theta = 1e-3\n",
    "traj_steps = 100\n",
    "old_q_table = q_table.copy() + 1\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    delta = np.sum((old_q_table - q_table) ** 2)\n",
    "    print(f\"iter {iter} Euclidean Distance:{delta}\")\n",
    "\n",
    "    old_q_table = q_table.copy()\n",
    "    for state in range(env.get_state_space_size()):\n",
    "        for action in range(env.get_action_space_size()):\n",
    "            traj = env.get_traj(state, action, policy, steps=traj_steps)\n",
    "\n",
    "            state, action, reward, next_state, next_action = traj[-1]\n",
    "            for k in range(traj_steps - 1, -1, -1):\n",
    "                tmp_state, tmp_action, tmp_score, _, _ = traj[k]\n",
    "                reward = reward * gamma + tmp_score\n",
    "                q_table[tmp_state][tmp_action] = reward\n",
    "\n",
    "    policy = np.eye(5)[np.argmax(q_table, axis=1)]\n",
    "    env.render_policy(policy)\n",
    "\n",
    "    if delta < theta:\n",
    "        print(f\"Converged at iteration {iter}\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e40f4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
