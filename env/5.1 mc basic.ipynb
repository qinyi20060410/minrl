{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dc6781a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from grid_world import GridWorld, Action\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "29f51b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.9\n",
    "rows = 5\n",
    "cols = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7314bd1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init grid:\n",
      "â¬œï¸â¬œï¸â¬œï¸â¬œï¸â¬œï¸\n",
      "â¬œï¸ğŸš«ğŸš«â¬œï¸â¬œï¸\n",
      "â¬œï¸â¬œï¸ğŸš«â¬œï¸â¬œï¸\n",
      "â¬œï¸ğŸš«âœ…ğŸš«â¬œï¸\n",
      "â¬œï¸ğŸš«â¬œï¸â¬œï¸â¬œï¸\n",
      "random policy:\n",
      "â¬†ï¸â¡ï¸â¬…ï¸â¡ï¸â¬†ï¸\n",
      "â¡ï¸â«ï¸â«ï¸â¬†ï¸ğŸ”„\n",
      "â¬‡ï¸â¡ï¸â«ï¸â¬†ï¸â¬†ï¸\n",
      "â¡ï¸â«ï¸âœ…â¬â¬…ï¸\n",
      "â¬†ï¸â¬â¬‡ï¸â¬…ï¸â¬‡ï¸\n"
     ]
    }
   ],
   "source": [
    "env = GridWorld(\n",
    "    desc=[\".....\", \".##..\", \"..#..\", \".#T#.\", \".#...\"],\n",
    "    forbidden_score=-10,\n",
    "    terminal_score=1,\n",
    ")\n",
    "print(\"init grid:\")\n",
    "env.render_grid()\n",
    "# print(\"init policy:\")\n",
    "state_values = np.zeros(rows * cols)\n",
    "q_table = np.zeros((rows * cols, len(Action)))\n",
    "# policy = np.argmax(q_table, axis=1)\n",
    "# env.render_policy(policy)\n",
    "\n",
    "print(\"random policy:\")\n",
    "# policy = np.random.choice(len(Action), size=rows * cols)\n",
    "policy = np.eye(cols)[np.random.randint(0, 5, size=(rows * cols))]\n",
    "env.render_policy(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "83c878d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0 Euclidean Distance:125.0\n",
      "q_table:\n",
      "[-99.99760947 -99.99760947   0.         -99.99760947   9.99976095]\n",
      "[  9.99976095   0.          -9.99976095 -99.99760947   0.        ]\n",
      "â¡ï¸â¡ï¸â¡ï¸â¡ï¸â¬‡ï¸\n",
      "â¬†ï¸â«ï¸â«ï¸â¬†ï¸â¬†ï¸\n",
      "â¬†ï¸â¬…ï¸â¬â¬†ï¸â¬†ï¸\n",
      "â¬†ï¸â©ï¸âœ…âªâ¬†ï¸\n",
      "â¬†ï¸â©ï¸â¬†ï¸â¡ï¸â¬†ï¸\n",
      "iter 1 Euclidean Distance:292486.0155906381\n",
      "q_table:\n",
      "[-99.99760947 -99.99760947   0.         -99.99760947   9.99976095]\n",
      "[  9.99976095   0.          -9.99976095 -99.99760947   0.        ]\n",
      "â¡ï¸â¡ï¸â¡ï¸â¡ï¸â¬‡ï¸\n",
      "â¬†ï¸â«ï¸â«ï¸â¬†ï¸â¬†ï¸\n",
      "â¬†ï¸â¬…ï¸â¬â¬†ï¸â¬†ï¸\n",
      "â¬†ï¸â©ï¸âœ…âªâ¬†ï¸\n",
      "â¬†ï¸â©ï¸â¬†ï¸â¡ï¸â¬†ï¸\n",
      "iter 2 Euclidean Distance:0.0\n",
      "q_table:\n",
      "[-99.99760947 -99.99760947   0.         -99.99760947   9.99976095]\n",
      "[  9.99976095   0.          -9.99976095 -99.99760947   0.        ]\n",
      "â¡ï¸â¡ï¸â¡ï¸â¡ï¸â¬‡ï¸\n",
      "â¬†ï¸â«ï¸â«ï¸â¬†ï¸â¬†ï¸\n",
      "â¬†ï¸â¬…ï¸â¬â¬†ï¸â¬†ï¸\n",
      "â¬†ï¸â©ï¸âœ…âªâ¬†ï¸\n",
      "â¬†ï¸â©ï¸â¬†ï¸â¡ï¸â¬†ï¸\n",
      "Converged at iteration 2\n"
     ]
    }
   ],
   "source": [
    "max_iters = 1000\n",
    "theta = 1e-3\n",
    "traj_steps = 100\n",
    "old_q_table = q_table.copy() + 1\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    delta = np.sum((old_q_table - q_table) ** 2)\n",
    "    print(f\"iter {iter} Euclidean Distance:{delta}\")\n",
    "\n",
    "    old_q_table= q_table.copy()\n",
    "    for state in range(env.get_state_space_size()):\n",
    "        for action in range(env.get_action_space_size()):\n",
    "            # [state, action, reward, next_state, next_action] in each element\n",
    "            traj = env.get_traj(state, action, policy, steps=traj_steps)\n",
    "\n",
    "            reward = traj[-1][2]\n",
    "            for k in range(traj_steps - 1, -1, -1):\n",
    "                reward = reward * gamma + traj[k][2]\n",
    "            q_table[state][action] = reward\n",
    "\n",
    "    print(f\"q_table:\\n{q_table[17]}\")\n",
    "    print(f\"{q_table[22]}\")\n",
    "    policy = np.eye(cols)[np.argmax(q_table, axis=1)]\n",
    "    env.render_policy(policy)\n",
    "\n",
    "    if delta < theta:\n",
    "        print(f\"Converged at iteration {iter}\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f7af4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b49d10b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
