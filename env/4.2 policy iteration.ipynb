{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73951d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from grid_world import GridWorld, Action\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb96cb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.9\n",
    "rows = 5\n",
    "cols = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b07380c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init grid:\n",
      "â¬œï¸â¬œï¸â¬œï¸â¬œï¸â¬œï¸\n",
      "â¬œï¸ğŸš«ğŸš«â¬œï¸â¬œï¸\n",
      "â¬œï¸â¬œï¸ğŸš«â¬œï¸â¬œï¸\n",
      "â¬œï¸ğŸš«âœ…ğŸš«â¬œï¸\n",
      "â¬œï¸ğŸš«â¬œï¸â¬œï¸â¬œï¸\n",
      "init policy:\n",
      "â¬†ï¸â¬†ï¸â¬†ï¸â¬†ï¸â¬†ï¸\n",
      "â¬†ï¸â«ï¸â«ï¸â¬†ï¸â¬†ï¸\n",
      "â¬†ï¸â¬†ï¸â«ï¸â¬†ï¸â¬†ï¸\n",
      "â¬†ï¸â«ï¸âœ…â«ï¸â¬†ï¸\n",
      "â¬†ï¸â«ï¸â¬†ï¸â¬†ï¸â¬†ï¸\n"
     ]
    }
   ],
   "source": [
    "env = GridWorld(\n",
    "    desc=[\".....\", \".##..\", \"..#..\", \".#T#.\", \".#...\"],\n",
    "    forbidden_score=-10,\n",
    "    terminal_score=1,\n",
    ")\n",
    "print(\"init grid:\")\n",
    "env.render_grid()\n",
    "print(\"init policy:\")\n",
    "state_values = np.zeros(rows * cols)\n",
    "q_table = np.zeros((rows * cols, len(Action)))\n",
    "policy = np.argmax(q_table, axis=1)\n",
    "env.render_policy(policy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844863d4",
   "metadata": {},
   "source": [
    "## random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a46ea33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â¬‡ï¸â¬†ï¸â¬†ï¸ğŸ”„â¬…ï¸\n",
      "â¬†ï¸â©ï¸â¬ğŸ”„â¬†ï¸\n",
      "â¬‡ï¸â¬‡ï¸â¬â¡ï¸â¬‡ï¸\n",
      "â¬…ï¸â«ï¸âœ…â¬â¬†ï¸\n",
      "â¬‡ï¸ğŸ”„â¬‡ï¸â¬‡ï¸â¬…ï¸\n"
     ]
    }
   ],
   "source": [
    "policy = np.random.choice(len(Action), size=rows * cols)\n",
    "env.render_policy(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263ef175",
   "metadata": {},
   "source": [
    "## Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5565ddb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0 Euclidean Distance:25.0\n",
      "state_values:\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0.]\n",
      "â¡ï¸â¡ï¸â¡ï¸â¡ï¸â¬‡ï¸\n",
      "â¬†ï¸â«ï¸â©ï¸â¡ï¸â¬‡ï¸\n",
      "â¬†ï¸â¬…ï¸â¬â¡ï¸â¬‡ï¸\n",
      "â¬†ï¸â©ï¸âœ…âªâ¬‡ï¸\n",
      "â¬†ï¸â©ï¸â¬†ï¸â¬…ï¸â¬…ï¸\n",
      "\n",
      "env:\n",
      "â¬œï¸â¬œï¸â¬œï¸â¬œï¸â¬œï¸\n",
      "â¬œï¸ğŸš«ğŸš«â¬œï¸â¬œï¸\n",
      "â¬œï¸â¬œï¸ğŸš«â¬œï¸â¬œï¸\n",
      "â¬œï¸ğŸš«âœ…ğŸš«â¬œï¸\n",
      "â¬œï¸ğŸš«â¬œï¸â¬œï¸â¬œï¸\n",
      "final state_values:\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0.]\n",
      "final policy:\n",
      "[1 1 1 1 2 0 0 1 1 2 0 3 2 1 2 0 1 4 3 2 0 1 0 3 3]\n",
      "â¡ï¸â¡ï¸â¡ï¸â¡ï¸â¬‡ï¸\n",
      "â¬†ï¸â«ï¸â©ï¸â¡ï¸â¬‡ï¸\n",
      "â¬†ï¸â¬…ï¸â¬â¡ï¸â¬‡ï¸\n",
      "â¬†ï¸â©ï¸âœ…âªâ¬‡ï¸\n",
      "â¬†ï¸â©ï¸â¬†ï¸â¬…ï¸â¬…ï¸\n"
     ]
    }
   ],
   "source": [
    "max_iters = 1000\n",
    "theta = 1e-3\n",
    "state_values = np.zeros(env.get_state_space_size())\n",
    "old_state_values = state_values + 1\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    delta = np.sum((old_state_values - state_values) ** 2)\n",
    "    print(f\"iter {iter} Euclidean Distance:{delta}\")\n",
    "\n",
    "    old_state_values = state_values.copy()\n",
    "    for state in range(env.get_state_space_size()):\n",
    "        \n",
    "        # kind 1\n",
    "        for action in range(env.get_action_space_size()):\n",
    "            reward, next_state = env.get_reward(state, action)\n",
    "            q_table[state][action] = reward + gamma * state_values[next_state]\n",
    "\n",
    "        # kind 2\n",
    "        # now_action = policy[state]\n",
    "        # reward, next_state = env.get_reward(state, now_action)\n",
    "        # state_values[state] = reward + gamma * old_state_values[next_state]\n",
    "\n",
    "    # state_values = np.max(q_table, axis=1)\n",
    "    # policy = np.argmax(q_table, axis=1)\n",
    "    print(f\"state_values:\\n{state_values}\")\n",
    "    env.render_policy(policy)\n",
    "\n",
    "    delta = np.sum((old_state_values - state_values) ** 2)\n",
    "    # print(f\"after iter {iter} Euclidean Distance:{delta}\")\n",
    "    if delta < theta:\n",
    "        break\n",
    "\n",
    "print(\"\\nenv:\")\n",
    "env.render_grid()\n",
    "print(f\"final state_values:\\n{state_values}\")\n",
    "print(f\"final policy:\\n{policy}\")\n",
    "env.render_policy(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d3c952",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcd7077",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
